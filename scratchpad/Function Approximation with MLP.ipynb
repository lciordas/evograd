{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation with Fully Connected Neural Networks\n",
    "\n",
    "Exploring how well a plain-vanilla fully connected neural network can approximate arbitrary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "\n",
    "from math import pi\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Use float64 for higher precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Function\n",
    "\n",
    "Define the function we want to approximate: `f(x, y) -> z` and evaluate it on a 2D grid to generate the training set.\n",
    "\n",
    "To experiment with different functions, modify the expression inside `target_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function we want to approximate: f(x, y) -> z\n",
    "    \n",
    "    Parameters:\n",
    "        x: Input array of shape (n_samples,)\n",
    "        y: Input array of shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "        Output array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    return np.exp(np.sin(pi*x) + y**2)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Generate training data on a 2D grid\n",
    "X_MIN = -1.0\n",
    "X_MAX = +1.0\n",
    "Y_MIN = -1.0\n",
    "Y_MAX = +1.0\n",
    "NPOINTS_PER_DIM = 100\n",
    "\n",
    "# Training set: regular grid\n",
    "Xs_train = np.linspace(X_MIN, X_MAX, NPOINTS_PER_DIM).astype(np.float64)\n",
    "Ys_train = np.linspace(Y_MIN, Y_MAX, NPOINTS_PER_DIM).astype(np.float64)\n",
    "x_grid_train, y_grid_train = np.meshgrid(Xs_train, Ys_train)\n",
    "\n",
    "x_flat_train = x_grid_train.flatten()\n",
    "y_flat_train = y_grid_train.flatten()\n",
    "z_flat_train = target_function(x_flat_train, y_flat_train).astype(np.float64)\n",
    "\n",
    "# Test set: offset by half the grid spacing (between training points)\n",
    "step_x = (X_MAX - X_MIN) / (NPOINTS_PER_DIM - 1)\n",
    "step_y = (Y_MAX - Y_MIN) / (NPOINTS_PER_DIM - 1)\n",
    "Xs_test = np.linspace(X_MIN + step_x/2, X_MAX - step_x/2, NPOINTS_PER_DIM - 1).astype(np.float64)\n",
    "Ys_test = np.linspace(Y_MIN + step_y/2, Y_MAX - step_y/2, NPOINTS_PER_DIM - 1).astype(np.float64)\n",
    "x_grid_test, y_grid_test = np.meshgrid(Xs_test, Ys_test)\n",
    "\n",
    "x_flat_test = x_grid_test.flatten()\n",
    "y_flat_test = y_grid_test.flatten()\n",
    "z_flat_test = target_function(x_flat_test, y_flat_test).astype(np.float64)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "X_train = torch.from_numpy(np.stack([x_flat_train, y_flat_train], axis=1)).to(device)\n",
    "Z_train = torch.from_numpy(z_flat_train).unsqueeze(1).to(device)\n",
    "\n",
    "X_test = torch.from_numpy(np.stack([x_flat_test, y_flat_test], axis=1)).to(device)\n",
    "Z_test = torch.from_numpy(z_flat_test).unsqueeze(1).to(device)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} points\")\n",
    "print(f\"Test set:     {X_test.shape[0]} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "A fully connected (dense) neural network with configurable hidden layers and activation function. The network takes 2 inputs (x, y) and produces 1 output (z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A configurable fully connected neural network for function approximation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_layers: list[int], activation: nn.Module):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim:     Number of input features\n",
    "            output_dim:    Number of output features\n",
    "            hidden_layers: List specifying number of neurons in each hidden layer\n",
    "            activation:    Activation function class to use between layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        layers   = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Supports Adam, L-BFGS, or hybrid (Adam then L-BFGS). Tracks best model and restores it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Training\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 27\n",
    "\n",
    "# configure the NN\n",
    "HIDDEN_LAYERS = [24, 24, 24]\n",
    "ACTIVATION    = nn.Tanh\n",
    "\n",
    "# configure optimizer\n",
    "OPTIMIZER_TYPE   = 'adam'  # 'adam', 'lbfgs', or 'hybrid'\n",
    "NUM_EPOCHS_ADAM  = 1000000\n",
    "NUM_EPOCHS_LBFGS =    1000\n",
    "LR_ADAM          =    1e-3\n",
    "LR_LBFGS         =     0.5\n",
    "\n",
    "if OPTIMIZER_TYPE == 'adam':\n",
    "    NUM_EPOCHS  = NUM_EPOCHS_ADAM\n",
    "    PRINT_EVERY = 10000\n",
    "elif OPTIMIZER_TYPE == 'lbfgs':\n",
    "    NUM_EPOCHS = NUM_EPOCHS_LBFGS\n",
    "    PRINT_EVERY = 100\n",
    "else:  # hybrid\n",
    "    NUM_EPOCHS  = NUM_EPOCHS_ADAM + NUM_EPOCHS_LBFGS\n",
    "    PRINT_EVERY = 1000\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create NN\n",
    "model = FullyConnectedNetwork(2, 1, HIDDEN_LAYERS, ACTIVATION).to(device)\n",
    "\n",
    "# Log status\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Network: 2 -> {' -> '.join(map(str, HIDDEN_LAYERS))} -> 1 | {n_params} params | {ACTIVATION.__name__}\")\n",
    "print(f\"Random seed: {SEED}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_epoch = 0\n",
    "loss_holder = [None]  # For L-BFGS closure\n",
    "current_optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Determine which optimizer to use\n",
    "    if OPTIMIZER_TYPE == 'adam':\n",
    "        use_adam = True\n",
    "    elif OPTIMIZER_TYPE == 'lbfgs':\n",
    "        use_adam = False\n",
    "    else:  # hybrid\n",
    "        use_adam = epoch < NUM_EPOCHS_ADAM\n",
    "    \n",
    "    # Initialize or switch optimizer\n",
    "    if epoch == 0 and (OPTIMIZER_TYPE == 'adam' or OPTIMIZER_TYPE == 'hybrid'):\n",
    "        current_optimizer = optim.Adam(model.parameters(), lr=LR_ADAM)\n",
    "        scheduler = ReduceLROnPlateau(current_optimizer, mode='min', factor=0.5, patience=200, min_lr=1e-6)\n",
    "    elif epoch == 0 and OPTIMIZER_TYPE == 'lbfgs':\n",
    "        current_optimizer = optim.LBFGS(model.parameters(), lr=LR_LBFGS, max_iter=20, history_size=100)\n",
    "    elif OPTIMIZER_TYPE == 'hybrid' and epoch == NUM_EPOCHS_ADAM:\n",
    "        print(f\"\\n--- Switching to L-BFGS at epoch {epoch} ---\\n\")\n",
    "        current_optimizer = optim.LBFGS(model.parameters(), lr=LR_LBFGS, max_iter=20, history_size=100)\n",
    "        scheduler = None\n",
    "    \n",
    "    if use_adam:\n",
    "        predictions = model(X_train)\n",
    "        loss = criterion(predictions, Z_train)\n",
    "        current_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        current_optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step(loss.detach())\n",
    "        current_train_loss = loss.item()\n",
    "        lrs.append(current_optimizer.param_groups[0]['lr'])\n",
    "    else:\n",
    "        def closure():\n",
    "            current_optimizer.zero_grad()\n",
    "            predictions = model(X_train)\n",
    "            loss = criterion(predictions, Z_train)\n",
    "            loss.backward()\n",
    "            loss_holder[0] = loss.item()\n",
    "            return loss\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            current_optimizer.step(closure)\n",
    "        current_train_loss = loss_holder[0]\n",
    "        lrs.append(LR_LBFGS)  # L-BFGS doesn't adapt LR the same way\n",
    "    \n",
    "    # Calculate test loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(X_test)\n",
    "        current_test_loss = criterion(test_predictions, Z_test).item()\n",
    "    \n",
    "    # Stop if NaN encountered\n",
    "    if np.isnan(current_train_loss):\n",
    "        print(f\"NaN detected at epoch {epoch + 1}, stopping training.\")\n",
    "        break\n",
    "    \n",
    "    train_losses.append(current_train_loss)\n",
    "    test_losses.append(current_test_loss)\n",
    "    \n",
    "    # Track best model (based on training loss)\n",
    "    if current_train_loss < best_loss:\n",
    "        best_loss = current_train_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    # Print progress\n",
    "    print_now = (epoch + 1) % PRINT_EVERY == 0\n",
    "    if OPTIMIZER_TYPE == 'hybrid' and epoch >= NUM_EPOCHS_ADAM:\n",
    "        print_now = (epoch - NUM_EPOCHS_ADAM + 1) % 100 == 0  # Print more frequently during L-BFGS\n",
    "    if print_now:\n",
    "        phase = \"Adam\" if use_adam else \"L-BFGS\"\n",
    "        print(f\"Epoch {epoch + 1:5d}/{NUM_EPOCHS} [{phase:6s}] | Train: {current_train_loss:.2e} | Test: {current_test_loss:.2e} | Best: {best_loss:.2e} | LR: {lrs[-1]:.2e}\")\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"\\nO9.19e-04ptimizer: {OPTIMIZER_TYPE}\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.2e}\")\n",
    "print(f\"Final test loss:  {test_losses[-1]:.2e}\")\n",
    "print(f\"Best train loss:  {best_loss:.2e} (epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z_pred_train = model(X_train).cpu().numpy().squeeze()\n",
    "    z_pred_test = model(X_test).cpu().numpy().squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Training and test loss\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(train_losses, label='Train')\n",
    "ax1.semilogy(test_losses, label='Test', alpha=0.7)\n",
    "if OPTIMIZER_TYPE == 'hybrid':\n",
    "    ax1.axvline(x=NUM_EPOCHS_ADAM, color='r', linestyle='--', label='Switch to L-BFGS')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (log scale)')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Learning rate over time\n",
    "ax2 = axes[1]\n",
    "ax2.semilogy(lrs)\n",
    "if OPTIMIZER_TYPE == 'hybrid':\n",
    "    ax2.axvline(x=NUM_EPOCHS_ADAM, color='r', linestyle='--', label='Switch to L-BFGS')\n",
    "    ax2.legend()\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate (log scale)')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics - Training set\n",
    "error_train = z_flat_train - z_pred_train\n",
    "print(\"Training Set:\")\n",
    "print(f\"  Root Mean Squared Error: {np.sqrt(np.mean(error_train**2)):.2e}\")\n",
    "print(f\"  Mean Absolute Error:     {np.mean(np.abs(error_train)):.2e}\")\n",
    "print(f\"  Max Absolute Error:      {np.max(np.abs(error_train)):.2e}\")\n",
    "\n",
    "# Summary statistics - Test set\n",
    "error_test = z_flat_test - z_pred_test\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Root Mean Squared Error: {np.sqrt(np.mean(error_test**2)):.2e}\")\n",
    "print(f\"  Mean Absolute Error:     {np.mean(np.abs(error_test)):.2e}\")\n",
    "print(f\"  Max Absolute Error:      {np.max(np.abs(error_test)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE vs Network Size\n",
    "\n",
    "############################\n",
    "# function: x * y\n",
    "# SEED            = 27\n",
    "# ACTIVATION      = nn.Tanh\n",
    "# OPTIMIZER_TYPE  = 'adam' \n",
    "# LR_ADAM         =  1e-3\n",
    "# ReduceLROnPlateau(current_optimizer, mode='min', factor=0.5, patience=200, min_lr=1e-6)\n",
    "############################\n",
    "\n",
    "# ==========================\n",
    "# LBFGS\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_LBFGS    = [      67,      105,      337,     1185,     4417,    17025,    66817,   252501]\n",
    "RMSE_2_LBFGS = [1.38e-03, 1.32e-03, 7.48e-04, 1.18e-03, 9.16e-04, 9.68e-04, 1.43e-03, 6.94e-04]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241, \n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601, \n",
    "# [250,250,250] => 126501\n",
    "N_3_LBFGS    = [      57,      177,     609,     1297,     2241,     4897,     8577,    20601,   126501]\n",
    "RMSE_3_LBFGS = [1.06e-03, 1.29e-03,9.88e-04, 1.37e-03, 1.49e-03, 9.89e-04, 7.88e-04, 1.25e-03, 1.15e-03]\n",
    "\n",
    "# Four layers\n",
    "# [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_LBFGS    = [     111,      881,     1897,     3297,     7249,    12737,    30701,   189251]\n",
    "RMSE_4_LBFGS = [1.40e-03, 7.45e-04, 1.09e-03, 8.59e-04, 9.53e-04, 7.71e-04, 1.59e-03, 6.55e-04]\n",
    "\n",
    "# =========================\n",
    "# NUM_EPOCHS_ADAM = 100,000\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_ADAM_100k    = [      67,      105,      337,     1185,     4417,    17025,    66817,   252501]\n",
    "RMSE_2_ADAM_100k = [4.76e-04, 4.95e-04, 2.91e-04, 1.88e-04, 1.30e-04, 9.04e-05, 8.39e-05, 2.33e-04]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241\n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601\n",
    "# [250,250,250] => 126501\n",
    "N_3_ADAM_100k    = [      57,      177,      609,     1297,     2241,     4897,     8577,    20601,   126501]\n",
    "RMSE_3_ADAM_100k = [5.70e-04, 4.82e-04, 1.77e-04, 2.36e-04, 3.51e-04, 2.81e-04, 1.78e-04, 1.16e-04, 1.23e-04]\n",
    "\n",
    "# Four layers\n",
    "# [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_ADAM_100k    = [     111,      881,     1897,     3297,     7249,    12737,    30701,   189251]\n",
    "RMSE_4_ADAM_100k = [9.19e-04, 5.86e-04, 3.90e-04, 3.29e-04, 3.74e-04, 4.15e-04, 3.85e-04, 2.99e-04]\n",
    "\n",
    "# ===========================\n",
    "# NUM_EPOCHS_ADAM = 1,000,000\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_ADAM_1M    = [      67,      105,      337,     1185,     4417,    17025,    66817,   252501]\n",
    "RMSE_2_ADAM_1M = [1.86e-04, 2.00e-04, 5.94e-05, 4.20e-05, 2.79e-05, 2.26e-05, 2.27e-05, 1.44e-05]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241\n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601\n",
    "# [250,250,250] => 126501\n",
    "N_3_ADAM_1M    = [      57,      177,      609,     1297,     2241,     4897,     8577,    20601,   126501]\n",
    "RMSE_3_ADAM_1M = [2.45e-04, 1.78e-04, 4.90e-05, 6.78e-05, 4.46e-05, 4.46e-05, 4.21e-05, 3.11e-05, 3.84e-05]\n",
    "\n",
    "# Four layers\n",
    "# [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_ADAM_1M    = [     111,      881,     1897,     3297,     7249,    12737,    30701,   189251]\n",
    "RMSE_4_ADAM_1M = [4.14e-04, 1.85e-04, 9.01e-05, 6.05e-05, 5.91e-05, 9.47e-05, 5.04e-05, 1.10e-04]\n",
    "\n",
    "# ============================\n",
    "\n",
    "# Plot settings\n",
    "Y_MIN = 0.75e-4  # Set to a value (e.g., 1e-5) to fix the lower y-axis limit, or None for auto\n",
    "Y_MAX = 1e-2  # Set to a value (e.g., 1e-2) to fix the upper y-axis limit, or None for auto\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(N_2_LBFGS, RMSE_2_LBFGS, 'o-', label='2 hidden layers')\n",
    "plt.loglog(N_3_LBFGS, RMSE_3_LBFGS, 's-', label='3 hidden layers')\n",
    "plt.loglog(N_4_LBFGS, RMSE_4_LBFGS, 's-', label='4 hidden layers')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Network Size: f(x,y) = x * y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "if Y_MIN is not None or Y_MAX is not None:\n",
    "    plt.ylim(Y_MIN, Y_MAX)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE vs Network Size\n",
    "\n",
    "############################\n",
    "# function: np.exp(np.sin(pi*x) + y**2)\n",
    "# SEED            = 27\n",
    "# ACTIVATION      = nn.Tanh\n",
    "# OPTIMIZER_TYPE  = 'adam' \n",
    "# LR_ADAM         =  1e-3\n",
    "# ReduceLROnPlateau(current_optimizer, mode='min', factor=0.5, patience=200, min_lr=1e-6)\n",
    "############################\n",
    "\n",
    "# ==========================\n",
    "# LBFGS\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_LBFGS    = [      67,      105,      337,     1185,     4417,    17025,    66817,   252501]\n",
    "RMSE_2_LBFGS = [3.37e-03, 3.80e-03, 1.45e-03, 1.37e-03, 1.82e-03, 1.61e-03, 1.48e-03, 1.58e-03]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241, \n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601, \n",
    "# [250,250,250] => 126501\n",
    "N_3_LBFGS    = [      57,      177,     609,     1297,    2241,     4897,     8577,    20601,   126501]\n",
    "RMSE_3_LBFGS = [4.53e-02, 1.93e-03, 1.58e-3, 1.60e-03, 1.24e-3, 1.26e-03, 8.96e-04, 7.70e-04, 1.26e-03]\n",
    "\n",
    "# Four layers\n",
    "# [  3,  4,  4,  3] =>    67, [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_LBFGS    = [      64,      111,      881,     1897,     3297,     7249,     12737,    30701,   189251]\n",
    "RMSE_4_LBFGS = [3.68e-02, 2.20e-03, 1.81e-03, 2.52e-03, 1.46e-03, 1.13e-03,  1.85e-04, 2.13e-04, 8.85e-04]\n",
    "\n",
    "# =========================\n",
    "# NUM_EPOCHS_ADAM = 100,000\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_ADAM_100k    = [      67,      105,      337,     1185,     4417,    17025,    66817,   252501]\n",
    "RMSE_2_ADAM_100k = [9.42e-03, 2.79e-03, 5.70e-03, 1.29e-03, 9.47e-04, 2.34e-03, 8.04e-04, 5.48e-04]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241\n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601\n",
    "# [250,250,250] => 126501\n",
    "N_3_ADAM_100k    = [      57,      177,     609,     1297,     2241,     4897,     8577,   20601,   126501]\n",
    "RMSE_3_ADAM_100k = [1.14e-02, 3.01e-03, 2.05e-3, 7.49e-04, 1.43e-03, 6.63e-04, 5.72e-04, 5.5e-04, 3.42e-04]\n",
    "\n",
    "# Four layers\n",
    "# [  3,  4,  4,  3] =>    67, [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_ADAM_100k    = [      64,      111,      881,     1897,     3297,     7249,    12737,    30701,   189251]\n",
    "RMSE_4_ADAM_100k = [1.13e-02, 3.51e-03, 1.44e-03, 1.60e-03, 8.67e-04, 7.48e-04, 7.18e-04, 5.07e-04, 2.88e-04]\n",
    "\n",
    "# ===========================\n",
    "# NUM_EPOCHS_ADAM = 1,000,000\n",
    "\n",
    "# Two layers\n",
    "# [6,6] => 67, [8,8] => 105, [16,16] => 337, [32,32] => 1185, [64,64] => 4417, \n",
    "# [128, 128] => 17025, [256,256] => 66817, [500,500] => 252501\n",
    "N_2_ADAM_1M    = [       67,      105,      337,     1185,    4417,    17025,    66817,    252501]\n",
    "RMSE_2_ADAM_1M = [ 5.44e-03, 8.97e-04, 9.14e-04, 3.39e-04, 3.07e-4, 2.03e-04, 1.82e-04,  1.52e-04]\n",
    "\n",
    "# Three layers\n",
    "# [4,4,4] => 57, [8,8,8] => 177, [16,16,16] => 609, [24,24,24] => 1297, [32,32,32] => 2241\n",
    "# [48,48,48] => 4897, [64,64,64] => 8577, [100,100,100] => 20601\n",
    "# [250,250,250] => 126501\n",
    "N_3_ADAM_1M    = [     57,      177,      609,     1297,     2241,     4897,     8577,    20601,   126501]\n",
    "RMSE_3_ADAM_1M = [4.07e-3, 1.23e-03, 7.65e-04, 2.06e-04, 2.38e-04, 1.58e-04, 1.54e-04, 1.48e-04, 8.96e-05]\n",
    "\n",
    "# Four layers\n",
    "# [  3,  4,  4,  3] =>    67, [ 5, 5, 5, 5] =>  111, \n",
    "# [ 16, 16, 16, 16] =>   881, [24,24,24,24] => 1897,\n",
    "# [ 32, 32, 32, 32] =>  3297, [48,48,48,48] => 7249\n",
    "# [ 64, 64, 64, 64] => 12737\n",
    "# [100,100,100,100] => 30701\n",
    "# [250,250,250,250] => 189251\n",
    "N_4_ADAM_1M    = [     64,       111,      881,     1897,      3297,     7249,    12737,    30701,   189251]\n",
    "RMSE_4_ADAM_1M = [4.63e-03, 1.43e-03, 4.33e-04, 2.49e-04,  2.36e-04, 1.85e-04, 2.13e-04, 1.32e-04, 9.41e-05]\n",
    "\n",
    "# ============================\n",
    "# NUM_EPOCHS_ADAM = 10,000,000\n",
    "\n",
    "# Four layers\n",
    "# [100,100,100,100] =>  30701\n",
    "# [250,250,250,250] => 189251\n",
    "#N_4    = [   30701,   189251]\n",
    "#RMSE_4 = [4.18e-05, 2.37e-05]\n",
    "\n",
    "# ============================\n",
    "\n",
    "# Plot settings\n",
    "Y_MIN = 0.75e-4  # Set to a value (e.g., 1e-5) to fix the lower y-axis limit, or None for auto\n",
    "Y_MAX = 1e-1  # Set to a value (e.g., 1e-2) to fix the upper y-axis limit, or None for auto\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(N_2_LBFGS, RMSE_2_LBFGS, 'o-', label='2 hidden layers')\n",
    "plt.loglog(N_3_LBFGS, RMSE_3_LBFGS, 's-', label='3 hidden layers')\n",
    "plt.loglog(N_4_LBFGS, RMSE_4_LBFGS, 's-', label='4 hidden layers')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Network Size: f(x,y) = exp(sin(pi*x) + y**2)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "if Y_MIN is not None or Y_MAX is not None:\n",
    "    plt.ylim(Y_MIN, Y_MAX)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
